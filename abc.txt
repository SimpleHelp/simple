1. Perform the following operations using Python on a data set : read data
from different formats(like csv, xls),indexing and selecting data, sort data,
describe attributes of data, checking data types of each column. (Use
Titanic Dataset).
------------------------------------

# Step 1: Import libraries
import pandas as pd

# Step 2: Read Titanic dataset (CSV file)

titanic_csv = pd.read_csv("titanic.csv")
print("CSV Data Loaded Successfully\n")
print(titanic_csv.head())

# Step 3: Read Titanic dataset (Excel file)

titanic_xls = pd.read_excel("titanic.xls")
print("\nExcel Data Loaded Successfully\n")
print(titanic_xls.head())

# Step 4: Indexing and Selecting Columns

print("\nSelecting 'Name' and 'Age' columns:\n")
print(titanic_csv[['Name', 'Age']].head())

print("\nSelecting first 5 rows using index:\n")
print(titanic_csv.iloc[0:5])

# Step 5: Sorting Data

print("\nData Sorted by Age:\n")
print(titanic_csv.sort_values(by="Age").head())

print("\nData Sorted by Fare (Descending):\n")
print(titanic_csv.sort_values(by="Fare", ascending=False).head())

# Step 6: Describe Data Attributes

print("\nDescribing Numeric Columns:\n")
print(titanic_csv.describe())

print("\nDescribing All Columns:\n")
print(titanic_csv.describe(include="all"))

# Step 7: Checking Data Types of Each Column

print("\nData Types of Columns:\n")
print(titanic_csv.dtypes)

--------------------------------------------------------------------------------------------
2. Perform the following operations using Python on the Telecom_Churn
dataset. Compute and display summary statistics for each feature available
in the dataset using separate commands for each statistic. (e.g. minimum
value, maximum value, mean, range, standard deviation, variance and
percentiles).
-------------------------------------

# Step 1: Import libraries

import pandas as pd

# Step 2: Load the Telecom_Churn dataset
# Make sure the file Telecom_Churn.csv is in the same folder

df = pd.read_csv("Telecom_Churn.csv")

# Display first 5 rows

print("\n=== DATASET LOADED ===\n")
print(df.head())

# Step 3: Select only numeric columns for statistical analysis

numeric_cols = df.select_dtypes(include=['int64', 'float64'])
print("\n=== NUMERIC FEATURES ===\n")
print(numeric_cols.columns)

# Step 4: Minimum value for each feature

print("\n=== MINIMUM VALUES ===\n")
print(numeric_cols.min())

# Step 5: Maximum value for each feature

print("\n=== MAXIMUM VALUES ===\n")
print(numeric_cols.max())

# Step 6: Mean for each feature

print("\n=== MEAN VALUES ===\n")
print(numeric_cols.mean())

# Step 7: Range (max - min) for each feature

print("\n=== RANGE (MAX - MIN) ===\n")
print(numeric_cols.max() - numeric_cols.min())

# Step 8: Standard Deviation

print("\n=== STANDARD DEVIATION ===\n")
print(numeric_cols.std())

# Step 9: Variance

print("\n=== VARIANCE ===\n")
print(numeric_cols.var())

# Step 10: Percentiles (25%, 50%, 75%)

print("\n=== PERCENTILES (25th, 50th, 75th) ===\n")
percentiles = numeric_cols.quantile([0.25, 0.5, 0.75])
print(percentiles)





-------------------------------------------------------------------------------------------
3. Perform the following operations using Python on the data set
House_Price Prediction dataset. Compute standard deviation, variance and
percentiles using separate commands, for each feature. Create a histogram
for each feature in the dataset to illustrate the feature distributions.

# ---- House Price Prediction Dataset Analysis ----

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

# 1. Load Dataset

df = pd.read_csv("House_Price_Prediction.csv")     # Change filename if needed

# 2. Select Numeric Features

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# 3. Handle Missing Numeric Values (Median Fill)

df[num_cols] = df[num_cols].fillna(df[num_cols].median())

# 4. Compute Summary Statistics

print("----- Standard Deviation -----")
print(df[num_cols].std(), "\n")

print("----- Variance -----")
print(df[num_cols].var(), "\n")

print("----- Percentiles (25th, 50th, 75th) -----")
print(df[num_cols].quantile([0.25, 0.50, 0.75]), "\n")

# 5. Create Histograms for All Numeric Features

os.makedirs("histograms", exist_ok=True)

for col in num_cols:
    plt.figure(figsize=(6, 4))
    plt.hist(df[col], bins=30)
    plt.title(f"Histogram of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.savefig(f"histograms/{col}_hist.png")  
    plt.show()
----------------------------------------------------------------------------------------
 4. Write a program to do: A dataset collected in a cosmetics shop showing
details of customers and whether or not they responded to a special offer
to buy a new lip-stick is shown in table below. (Implement step by step
using commands - Dont use library) Use this dataset to build a decision
tree, with Buys as the target variable, to help in buying lipsticks in the
future. Find the root node of the decision tree.
-------------------------------------------------

import math


# 1. Sample lipstick dataset (same as given)


data = [
    {"Age": "Young",  "Income": "High",   "Loyalty": "NonMember", "Buys": "No"},
    {"Age": "Young",  "Income": "High",   "Loyalty": "Member",    "Buys": "No"},
    {"Age": "Young",  "Income": "Medium", "Loyalty": "NonMember", "Buys": "Yes"},
    {"Age": "Middle", "Income": "High",   "Loyalty": "NonMember", "Buys": "Yes"},
    {"Age": "Senior", "Income": "Low",    "Loyalty": "Member",    "Buys": "Yes"},
    {"Age": "Senior", "Income": "Low",    "Loyalty": "NonMember", "Buys": "No"},
    {"Age": "Senior", "Income": "Medium", "Loyalty": "NonMember", "Buys": "Yes"},
    {"Age": "Middle", "Income": "Low",    "Loyalty": "Member",    "Buys": "Yes"},
    {"Age": "Middle", "Income": "Medium", "Loyalty": "NonMember", "Buys": "Yes"},
    {"Age": "Young",  "Income": "Low",    "Loyalty": "Member",    "Buys": "No"},
]

target_attr = "Buys"



# 2. Return values of a column

def get_column(data, attr):
    return [row[attr] for row in data]


# 3. Entropy function

def entropy(class_values):
    total = len(class_values)
    value_counts = {}
    
    # count frequency of labels
    for v in class_values:
        value_counts[v] = value_counts.get(v, 0) + 1

    ent = 0.0
    for count in value_counts.values():
        p = count / total
        ent -= p * math.log2(p)

    return ent



# 4. Information Gain

def information_gain(data, attr, target_attr):
    total_entropy = entropy(get_column(data, target_attr))
    total_len = len(data)

    values = set(get_column(data, attr))

    weighted_entropy = 0
    for v in values:
        subset = [row for row in data if row[attr] == v]
        subset_labels = get_column(subset, target_attr)
        subset_entropy = entropy(subset_labels)
        weight = len(subset) / total_len
        weighted_entropy += weight * subset_entropy

    return total_entropy - weighted_entropy


# 5. Find best attribute (max IG)

def find_best_attribute(data, target_attr):
    attributes = list(data[0].keys())
    attributes.remove(target_attr)

    best_attr = None
    best_ig = -1

    print("Information Gain for each attribute:\n")

    for attr in attributes:
        ig = information_gain(data, attr, target_attr)
        print(f"IG({attr}) = {ig:.4f}")
        if ig > best_ig:
            best_ig = ig
            best_attr = attr

    return best_attr, best_ig


# 6. Main driver

best_attr, best_ig = find_best_attribute(data, target_attr)

print("\n====================================")
print(" ROOT NODE of the decision tree is:")
print("  Attribute:", best_attr)
print("  Information Gain:", best_ig)
print("====================================")

---------------------------------------------------------------------------------------
5. Write a program to do: A dataset collected in a cosmetics shop showing
details of customers and whether or not they responded to a special offer
to buy a new lip-stick is shown in table below. (Use library commands)
According to the decision tree you have made from the previous training
data set, what is the decision for the test data: [Age &lt; 21, Income = Low,
Gender = Female, Marital Status = Married]?
------------------------------------------
# Step 1: Import libraries

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_text

# Step 2: Import dataset (Make sure cosmetics.csv is in the same folder)


# Age, Income, Gender, Marital, Responded
df = pd.read_csv("cosmetics.csv")

print("\n=== DATASET LOADED SUCCESSFULLY ===\n")
print(df)

# Step 3: Convert categorical columns to numerical using one-hot encoding

X = pd.get_dummies(df.drop(columns=['Responded']))
y = (df['Responded'] == 'Yes').astype(int)

# Step 4: Train Decision Tree

clf = DecisionTreeClassifier(random_state=0)
clf.fit(X, y)

# Step 5: Show decision tree rules

print("\n=== DECISION TREE RULES ===\n")
print(export_text(clf, feature_names=list(X.columns)))

# Step 6: Test data given in the question

test_data = {
    'Age': '<21',
    'Income': 'Low',
    'Gender': 'Female',
    'Marital': 'Married'
}

test_df = pd.DataFrame([test_data])
test_X = pd.get_dummies(test_df)

for col in X.columns:
    if col not in test_X.columns:
        test_X[col] = 0
test_X = test_X[X.columns]

# Step 7: Predict

prediction = clf.predict(test_X)[0]
proba = clf.predict_proba(test_X)[0]

print("\n=== TEST CASE ===")
print(test_df)

print("\nPrediction (1 = Yes, 0 = No):", prediction)
print("Probability → No =", proba[0], ", Yes =", proba[1])

if prediction == 1:
    print("\nFINAL DECISION: Customer WILL respond to the lipstick offer.")
else:
    print("\nFINAL DECISION: Customer will NOT respond to the lipstick offer.")


----------------------------------------------------------------------------------------
6. Write a program to do: A dataset collected in a cosmetics shop showing
details of customers and whether or not they responded to a special offer
to buy a new lip-stick is shown in table below. (Use library commands)
According to the decision tree you have made from the previous training
data set, what is the decision for the test data: [Age &gt; 35, Income =
Medium, Gender = Female, Marital Status = Married]?
----------------------------------------

# Step 1: Import libraries

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_text

# Step 2: Create the dataset (replace with your own table if needed)

data = [
    {'Age': '<=35', 'Income':'High',   'Gender':'Female', 'Marital':'Single', 'Responded':'No'},
    {'Age': '<=35', 'Income':'High',   'Gender':'Male',   'Marital':'Single', 'Responded':'No'},
    {'Age': '>35',  'Income':'Low',    'Gender':'Female', 'Marital':'Married','Responded':'Yes'},
    {'Age': '>35',  'Income':'Medium', 'Gender':'Female', 'Marital':'Married','Responded':'Yes'},
    {'Age': '<=35', 'Income':'Medium', 'Gender':'Female', 'Marital':'Married','Responded':'No'},
    {'Age': '>35',  'Income':'Medium', 'Gender':'Male',   'Marital':'Married','Responded':'Yes'},
    {'Age': '<=35', 'Income':'Low',    'Gender':'Male',   'Marital':'Single', 'Responded':'No'},
    {'Age': '>35',  'Income':'High',   'Gender':'Female', 'Marital':'Married','Responded':'Yes'},
    {'Age': '<=35', 'Income':'Medium', 'Gender':'Male',   'Marital':'Single', 'Responded':'No'},
    {'Age': '>35',  'Income':'Medium', 'Gender':'Female', 'Marital':'Single', 'Responded':'Yes'},
    {'Age': '<=35', 'Income':'Low',    'Gender':'Female', 'Marital':'Married','Responded':'No'},
    {'Age': '>35',  'Income':'High',   'Gender':'Male',   'Marital':'Married','Responded':'Yes'}
]

df = pd.DataFrame(data)

# Step 3: One-hot encode categorical data

X = pd.get_dummies(df.drop(columns=['Responded']))
y = (df['Responded'] == 'Yes').astype(int)

# Step 4: Train the Decision Tree

clf = DecisionTreeClassifier(random_state=0)
clf.fit(X, y)

# Step 5: Display decision tree rules

feature_names = list(X.columns)
print("\nDecision Tree Rules:\n")
print(export_text(clf, feature_names=feature_names))

# Step 6: Prepare test instance

test = {'Age': '>35', 'Income': 'Medium', 'Gender': 'Female', 'Marital': 'Married'}
test_df = pd.DataFrame([test])
test_X = pd.get_dummies(test_df)

# Align test columns with training input columns

for col in X.columns:
    if col not in test_X.columns:
        test_X[col] = 0
test_X = test_X[X.columns]

# Step 7: Predict

prediction = clf.predict(test_X)[0]
proba = clf.predict_proba(test_X)[0]

print("\nTest Case:", test)
print("\nPrediction (Yes = 1, No = 0):", prediction)
print("Probability → No:", proba[0], ", Yes:", proba[1])


----------------------------------------------------------------------------------------
7. Write a program to do: A dataset collected in a cosmetics shop showing
details of customers and whether or not they responded to a special offer
to buy a new lip-stick is shown in table below. (Use library commands)
According to the decision tree you have made from the previous training
data set, what is the decision for the test data: [Age &gt; 35, Income =
Medium, Gender = Female, Marital Status = Married]?

# Step 1: Import necessary libraries

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier

# Step 2: Load dataset (replace with your actual assignment table)

data = pd.DataFrame({
    'Age': ['<=20','21-35','21-35','<=20','36-50','21-35'],
    'Income': ['High','High','Low','Medium','Medium','Low'],
    'Gender': ['Female','Female','Male','Male','Female','Male'],
    'MaritalStatus': ['Single','Married','Single','Married','Single','Married'],
    'Buys': ['No','Yes','Yes','No','Yes','No']        
})

# Step 3: Label encode (convert text → numbers)

le = LabelEncoder()
for col in data.columns:
    data[col] = le.fit_transform(data[col])

# Step 4: Split into features and output

X = data.drop('Buys', axis=1)
y = data['Buys']

# Step 5: Train Decision Tree

model = DecisionTreeClassifier()
model.fit(X, y)

# Step 6: Test data input

test = pd.DataFrame({
    'Age': ['>35'],
    'Income': ['Medium'],
    'Gender': ['Female'],
    'MaritalStatus': ['Married']
})

# Step 7: Encode test data

for col in test.columns:
    test[col] = le.fit_transform(test[col])

# Step 8: Predict for test case

prediction = model.predict(test)[0]

print("Final Decision for Test Case:",
      "Buys" if prediction == 1 else "Does NOT Buy")


--------------------------------------------------------------------------------------
8. Write a program to do: A dataset collected in a cosmetics shop showing
details of customers and whether or not they responded to a special offer
to buy a new lip-stick is shown in table below. (Use library commands)
According to the decision tree you have made from the previous training
data set, what is the decision for the test data: [Age = 21-35, Income =
Low, Gender = Male, Marital Status = Married]?

# Step 1: Import libraries

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier

# Step 2: Load dataset (replace with your actual table)

data = pd.DataFrame({
    'Age': ['<=20','21-35','21-35','<=20','36-50','21-35'],
    'Income': ['High','High','Low','Medium','Medium','Low'],
    'Gender': ['Female','Female','Male','Male','Female','Male'],
    'MaritalStatus': ['Single','Married','Single','Married','Single','Married'],
    'Buys': ['No','Yes','Yes','No','Yes','No']   # target column
})

# Step 3: Encode categorical values

le = LabelEncoder()
for col in data.columns:
    data[col] = le.fit_transform(data[col])

# Step 4: Split features and target

X = data.drop('Buys', axis=1)
y = data['Buys']

# Step 5: Train the Decision Tree

model = DecisionTreeClassifier()
model.fit(X, y)

# Step 6: Test input

test = pd.DataFrame({
    'Age': ['21-35'],
    'Income': ['Low'],
    'Gender': ['Male'],
    'MaritalStatus': ['Married']
})

# Step 7: Encode test values

for col in test.columns:
    test[col] = le.fit_transform(test[col])

# Step 8: Predict

prediction = model.predict(test)[0]
print("Final Decision for the Test Case:", "Buys" if prediction==1 else "Does NOT Buy")


--------------------------------------------------------------------------------------
9. Write a program to do the following: You have given a collection of 8
points. P1=[0.1,0.6] P2=[0.15,0.71] P3=[0.08,0.9] P4=[0.16, 0.85]
P5=[0.2,0.3] P6=[0.25,0.5] P7=[0.24,0.1] P8=[0.3,0.2]. Perform the k-mean
clustering with initial centroids as m1=P1 =Cluster#1=C1 and
m2=P8=cluster#2=C2. Answer the following 1] Which cluster does P6
belong to? 2] What is the population of a cluster around m2? 3] What is
the updated value of m1 and m2?



# Step 1: Define all points

import math


points = {
    "P1": (0.1, 0.6),
    "P2": (0.15, 0.71),
    "P3": (0.08, 0.90),
    "P4": (0.16, 0.85),
    "P5": (0.2, 0.3),
    "P6": (0.25, 0.5),
    "P7": (0.24, 0.1),
    "P8": (0.3, 0.2)
}

# Initial centroids

m1 = points["P1"]   
m2 = points["P8"]   


# Step 2: Euclidean distance function

def distance(a, b):
    return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)


# Step 3: Assign points to clusters


C1 = []
C2 = []

for p, coord in points.items():
    d1 = distance(coord, m1)
    d2 = distance(coord, m2)
    
    if d1 < d2:
        C1.append(p)
    else:
        C2.append(p)


# Step 4: Update centroids

def compute_mean(cluster):
    x_vals = [points[p][0] for p in cluster]
    y_vals = [points[p][1] for p in cluster]
    return (sum(x_vals) / len(x_vals), sum(y_vals) / len(y_vals))

new_m1 = compute_mean(C1)
new_m2 = compute_mean(C2)


# Step 5: Print results

print("Cluster C1:", C1)
print("Cluster C2:", C2)

print("\nP6 belongs to:", "C1" if "P6" in C1 else "C2")
print("Population around m2 (C2):", len(C2))

print("\nUpdated Centroid m1 =", new_m1)
print("Updated Centroid m2 =", new_m2)

-------------------------------------------------------------------------------------
10. Write a program to do the following: You have given a collection of 8
points. P1=[2, 10] P2=[2, 5] P3=[8, 4] P4=[5, 8] P5=[7,5] P6=[6, 4] P7=[1, 2]
P8=[4, 9]. Perform the k-mean clustering with initial centroids as m1=P1
=Cluster#1=C1 and m2=P4=cluster#2=C2, m3=P7 =Cluster#3=C3. Answer
the following 1] Which cluster does P6 belong to? 2] What is the
population of a cluster around m3? 3] What is the updated value of m1,
m2, m3?


import math

# Step 1: Define points and initial centroids
points = {
    "P1": (2, 10),
    "P2": (2, 5),
    "P3": (8, 4),
    "P4": (5, 8),
    "P5": (7, 5),
    "P6": (6, 4),
    "P7": (1, 2),
    "P8": (4, 9)
}

m1 = points["P1"]
m2 = points["P4"]
m3 = points["P7"]

# Step 2: Distance function
def distance(a, b):
    return math.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)

# Step 3: Assign points to clusters
C1, C2, C3 = [], [], []

for p, coord in points.items():
    d1 = distance(coord, m1)
    d2 = distance(coord, m2)
    d3 = distance(coord, m3)
    min_d = min(d1, d2, d3)
    if min_d == d1:
        C1.append(p)
    elif min_d == d2:
        C2.append(p)
    else:
        C3.append(p)

# Step 4: Compute new centroids
def compute_mean(cluster):
    x_vals = [points[p][0] for p in cluster]
    y_vals = [points[p][1] for p in cluster]
    return (sum(x_vals)/len(x_vals), sum(y_vals)/len(y_vals))

new_m1 = compute_mean(C1)
new_m2 = compute_mean(C2)
new_m3 = compute_mean(C3)

# Step 5: Output
print("Cluster C1:", C1)
print("Cluster C2:", C2)
print("Cluster C3:", C3)

print("P6 belongs to:", "C1" if "P6" in C1 else ("C2" if "P6" in C2 else "C3"))
print("Population around m3 (C3):", len(C3))

print("Updated m1 =", new_m1)
print("Updated m2 =", new_m2)
print("Updated m3 =", new_m3)

---------------------------------------------------------------------------------------
11. Use Iris flower dataset and perform following :
1. List down the features and their types (e.g., numeric, nominal)
available in the dataset. 2. Create a histogram for each feature in the
dataset to illustrate the feature distributions.
---------------

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Load Iris dataset

iris = sns.load_dataset("iris")

# 1. List features & their types

print("=== FEATURES AND THEIR DATA TYPES ===")
print(iris.dtypes)

# 2. Create histogram for each feature

numeric_features = iris.select_dtypes(include=["float64", "int64"]).columns

for feature in numeric_features:
    plt.figure(figsize=(6,4))
    sns.histplot(data=iris, x=feature, kde=True)
    plt.title(f"Distribution of {feature}")
    plt.xlabel(feature)
    plt.ylabel("Frequency")
    plt.show()

----------------------------------------------------------------------------------------
12.Use Iris flower dataset and perform following :
1. Create a box plot for each feature in the dataset.
2. Identify and discuss distributions and identify outliers from them.

import seaborn as sns
import matplotlib.pyplot as plt

# Load Iris dataset

iris = sns.load_dataset("iris")

# 1. Create box plot for each numeric feature

numeric_features = ["sepal_length", "sepal_width", "petal_length", "petal_width"]

for feature in numeric_features:
    plt.figure(figsize=(6,4))
    sns.boxplot(data=iris, x=feature)
    plt.title(f"Boxplot of {feature}")
    plt.xlabel(feature)
    plt.show()

# Print summary statistics (useful for identifying outliers)

print("\n=== SUMMARY STATISTICS ===")
print(iris[numeric_features].describe())


-----------------------------------------------------------------------------------------
13. Use the covid_vaccine_statewise.csv dataset and perform the
following analytics.
a. Describe the dataset
b. Number of persons state wise vaccinated for first dose in India
c. Number of persons state wise vaccinated for second dose in India
---------------------------

import pandas as pd

# Load dataset

df = pd.read_csv("covid_vaccine_statewise.csv")

#(a) Describe the dataset 

print("===== DATASET HEAD =====")
print(df.head(), "\n")

print("===== DATASET INFO =====")
print(df.info(), "\n")

print("===== DESCRIPTIVE STATISTICS =====")
print(df.describe(include='all'), "\n")

#(b) State-wise First Dose Vaccination 

first_dose = df.groupby("State")["First Dose Administered"].sum().sort_values(ascending=False)

print("===== STATE-WISE FIRST DOSE VACCINATION =====")
print(first_dose, "\n")

# (c) State-wise Second Dose Vaccination

second_dose = df.groupby("State")["Second Dose Administered"].sum().sort_values(ascending=False)

print("===== STATE-WISE SECOND DOSE VACCINATION =====")
print(second_dose, "\n")


-----------------------------------------------------------------------------------------
14. Use the covid_vaccine_statewise.csv dataset and perform the
following analytics.
A. Describe the dataset.
B. Number of Males vaccinated
C.. Number of females vaccinated
-------------------------------

import pandas as pd

# Load dataset

df = pd.read_csv("covid_vaccine_statewise.csv")

# A. Describe the dataset 

print("===== FIRST 5 ROWS =====")
print(df.head(), "\n")

print("===== DATASET INFO =====")
print(df.info(), "\n")

print("===== SUMMARY STATISTICS =====")
print(df.describe(include='all'), "\n")

# B. Total Males Vaccinated 

total_males = df["Male(Individuals Vaccinated)"].sum()
print("===== TOTAL MALES VACCINATED IN INDIA =====")
print(total_males, "\n")

# C. Total Females Vaccinated 

total_females = df["Female(Individuals Vaccinated)"].sum()
print("===== TOTAL FEMALES VACCINATED IN INDIA =====")
print(total_females, "\n")


-----------------------------------------------------------------------------------------
15. Use the dataset &#39;titanic&#39;. The dataset contains 891 rows and contains
information about the passengers who boarded the unfortunate Titanic
ship. Use the Seaborn library to see if we can find any patterns in the data.
--------------------------

import seaborn as sns
import matplotlib.pyplot as plt

# Load Titanic dataset
titanic = sns.load_dataset("titanic")

print(titanic.head())
print("\nDataset Info:")
print(titanic.info())

# 1. Survival Count (How many survived vs died)

plt.figure(figsize=(6,4))
sns.countplot(data=titanic, x="survived")
plt.title("Count of Survived vs Not Survived")
plt.show()

# 2. Survival rate by Gender

plt.figure(figsize=(6,4))
sns.countplot(data=titanic, x="***", hue="survived")
plt.title("Survival by Gender")
plt.show()

# 3. Survival rate by Passenger Class (Pclass)

plt.figure(figsize=(6,4))
sns.countplot(data=titanic, x="pclass", hue="survived")
plt.title("Survival by Passenger Class")
plt.show()

# 4. Distribution of Age

plt.figure(figsize=(6,4))
sns.histplot(titanic["age"], kde=True)
plt.title("Age Distribution of Passengers")
plt.show()

# 5. Survival by Age

plt.figure(figsize=(6,4))
sns.boxplot(data=titanic, x="survived", y="age")
plt.title("Age vs Survival")
plt.show()

# 6. Fare distribution by Class

plt.figure(figsize=(6,4))
sns.boxplot(data=titanic, x="pclass", y="fare")
plt.title("Fare Distribution Across Classes")
plt.show()

# 7. Relationship: Fare vs Age (Colored by Survival)

plt.figure(figsize=(6,4))
sns.scatterplot(data=titanic, x="age", y="fare", hue="survived")
plt.title("Fare vs Age Colored by Survival")
plt.show()

# 8. Heatmap of Correlation

plt.figure(figsize=(8,5))
sns.heatmap(titanic.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

-------------------------------------------------------------------------------------------
16. Use the inbuilt dataset &#39;titanic&#39;. The dataset contains 891 rows and
contains information about the passengers who boarded the unfortunate
Titanic ship. Write a code to check how the price of the ticket (column
name: &#39;fare&#39;) for each passenger is distributed by plotting a histogram.
--------------------------

import seaborn as sns
import matplotlib.pyplot as plt

# Load inbuilt Titanic dataset
titanic = sns.load_dataset("titanic")

# Show first rows (optional)
print(titanic.head())

# Plot histogram for 'fare'
plt.hist(titanic["fare"], bins=30)
plt.xlabel("Fare")
plt.ylabel("Frequency")
plt.title("Distribution of Passenger Fare")
plt.show()

---------------------------------------------------------------------------------------------
17. Compute Accuracy, Error rate, Precision, Recall for following confusion
matrix ( Use formula for each)

True Positives (TPs): 1 False Positives (FPs): 1
False Negatives (FNs): 8 True Negatives (TNs): 90
-----------------------

# Confusion matrix values

TP = 1
FP = 1
FN = 8
TN = 90

# Calculations

accuracy = (TP + TN) / (TP + TN + FP + FN)
error_rate = (FP + FN) / (TP + TN + FP + FN)
precision = TP / (TP + FP)
recall = TP / (TP + FN)

# Print results

print("Accuracy :", round(accuracy, 4))
print("Error Rate :", round(error_rate, 4))
print("Precision :", round(precision, 4))
print("Recall :", round(recall, 4))


-----------------------------------------------------------------------------------------
18. Use House_Price prediction dataset. Provide summary statistics (mean,
median, minimum, maximum, standard deviation) of variables (categorical
vs quantitative) such as- For example, if categorical variable is age groups
and quantitative variable is income, then provide summary statistics of
income grouped by the age groups.
------------------------------------------

import pandas as pd

# Load dataset
df = pd.read_csv("House Data.csv")

# Choose categorical and numeric variable
categorical = "Location"          # change if needed
numerical = "Price"

# Group and calculate summary statistics
summary = df.groupby(categorical)[numerical].agg(
    Mean="mean",
    Median="median",
    Minimum="min",
    Maximum="max",
    Std_Deviation="std"
)

print(summary)
summary.to_csv("Summary_Stats_By_Location.csv")
print("\nSaved as Summary_Stats_By_Location.csv")

OR

import pandas as pd

# Step 1: Load the House Price dataset
df = pd.read_csv("house_price.csv")

# Step 2: Identify categorical and quantitative variables
categorical_cols = df.select_dtypes(include=['object']).columns
quantitative_cols = df.select_dtypes(include=['int64', 'float64']).columns

print("\nCategorical Variables:", list(categorical_cols))
print("Quantitative Variables:", list(quantitative_cols))

# Step 3: Choose one example categorical variable
# (You can change 'Neighborhood' to any categorical column in your dataset)
cat_var = categorical_cols[0]   # Taking the first categorical column for demonstration

print(f"\n\n=== Summary Statistics of Quantitative Variables Grouped by '{cat_var}' ===")

# Step 4: Compute summary statistics grouped by categorical variable
summary = df.groupby(cat_var)[quantitative_cols].agg(['mean', 'median', 'min', 'max', 'std'])

print(summary)



-----------------------------------------------------------------------------------------
19. Write a Python program to display some basic statistical details like
percentile, mean, standard deviation etc (Use python and pandas
commands) the species of ‘Iris-setosa’, ‘Iris-versicolor’ and ‘Iris-versicolor’
of iris.csv dataset.
------------------------------------------
import pandas as pd

# Load dataset
df = pd.read_csv("iris.csv")     # file must be in same folder

species_list = df["Species"].unique()

for sp in species_list:
    print(f"\n===== Statistical Summary for {sp} =====")
    subset = df[df["Species"] == sp]

    for col in ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]:
        print(f"\n Feature: {col}")
        print(f"Mean               : {subset[col].mean():.3f}")
        print(f"Median             : {subset[col].median():.3f}")
        print(f"Std Deviation      : {subset[col].std():.3f}")
        print(f"Minimum            : {subset[col].min():.3f}")
        print(f"25th Percentile    : {subset[col].quantile(0.25):.3f}")
        print(f"50th Percentile    : {subset[col].quantile(0.50):.3f}")
        print(f"75th Percentile    : {subset[col].quantile(0.75):.3f}")
        print(f"Maximum            : {subset[col].max():.3f}")


OR 

import pandas as pd

# Step 1: Load the iris dataset
df = pd.read_csv("iris.csv")

# Step 2: List of species to analyze
species_list = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']

# Step 3: For each species, display statistics
for sp in species_list:
    print("\n==============================")
    print("  Statistical Details for:", sp)
    print("==============================")
    
    # Filter data for the species
    sp_df = df[df['species'] == sp]
    
    # Display mean
    print("\nMean values:")
    print(sp_df.mean(numeric_only=True))
    
    # Display standard deviation
    print("\nStandard Deviation:")
    print(sp_df.std(numeric_only=True))
    
    # Display variance
    print("\nVariance:")
    print(sp_df.var(numeric_only=True))
    
    # Display percentiles (25th, 50th, 75th)
    print("\nPercentiles (25%, 50%, 75%):")
    print(sp_df.quantile([0.25, 0.50, 0.75]))






-----------------------------------------------------------------------------------------
20. Write a program to cluster a set of points using K-means for IRIS
dataset. Consider, K=3, clusters. Consider Euclidean distance as the
distance measure. Randomly initialize a cluster mean as one of the data
points. Iterate at least for 10 iterations. After iterations are over, print the
final cluster means for each of the clusters.
------------------------------------------

import pandas as pd
import numpy as np
import random
import math


# 1. Load IRIS dataset (CSV file must be in same folder)

df = pd.read_csv("iris.csv")   # Columns: sepal_length, sepal_width, petal_length, petal_width, class
data = df.iloc[:, 0:4].values   # Only numeric features


# 2. Euclidean distance function

def euclidean(p, q):
    return math.sqrt(sum((p[i] - q[i]) ** 2 for i in range(len(p))))


# 3. Initialize K = 3 cluster means randomly from data points

K = 3
means = []
for _ in range(K):
    means.append(random.choice(data).tolist())

print("Initial Means:")
for i, m in enumerate(means):
    print(f"Cluster {i+1}:", m)


# 4. Run K-means for at least 10 iterations

for iteration in range(10):
    clusters = {i: [] for i in range(K)}

    # Assign each point to the nearest mean
    for point in data:
        distances = [euclidean(point, means[i]) for i in range(K)]
        cluster_index = distances.index(min(distances))
        clusters[cluster_index].append(point)

    # Update means
    for i in range(K):
        if len(clusters[i]) > 0:
            means[i] = np.mean(clusters[i], axis=0).tolist()

    print(f"\nIteration {iteration+1} complete.")


# 5. Print Final Cluster Means after 10 iterations

print("\nFINAL CLUSTER MEANS:")
for i, m in enumerate(means):
    print(f"Cluster {i+1} Mean: {m}")

------------------------------------------------------------------------------------
21. Write a program to cluster a set of points using K-means for IRIS
dataset. Consider, K=4, clusters. Consider Euclidean distance as the
distance measure. Randomly initialize a cluster mean as one of the data
points. Iterate at least for 10 iterations. After iterations are over, print the
final cluster means for each of the clusters.
----------------

import pandas as pd
import numpy as np
import random
import math


# 1. Load IRIS dataset (iris.csv must be in the same folder)

df = pd.read_csv("iris.csv")   # Only first 4 columns are numeric features
data = df.iloc[:, 0:4].values


# 2. Euclidean Distance Function

def euclidean(p, q):
    return math.sqrt(sum((p[i] - q[i]) ** 2 for i in range(len(p))))


# 3. Initialize K = 4 Means (random points from dataset)

K = 4
means = []
for _ in range(K):
    means.append(random.choice(data).tolist())

print("Initial Means:")
for i, m in enumerate(means):
    print(f"Cluster {i+1}: {m}")


# 4. Run K-means for 10 iterations

for iteration in range(10):
    # Create empty clusters
    clusters = {i: [] for i in range(K)}

    # Assign points to nearest mean
    for point in data:
        distances = [euclidean(point, means[i]) for i in range(K)]
        cluster_index = distances.index(min(distances))
        clusters[cluster_index].append(point)

    # Update cluster means
    for i in range(K):
        if len(clusters[i]) > 0:
            means[i] = np.mean(clusters[i], axis=0).tolist()

    print(f"\nIteration {iteration+1} Completed")


# 5. Print Final Cluster Means

print("\nFINAL CLUSTER MEANS (K = 4):")
for i, m in enumerate(means):
    print(f"Cluster {i+1} Mean: {m}")

----------------------------------------------------------------------------------
22. Compute Accuracy, Error rate, Precision, Recall for the following
confusion matrix.
Actual Class\Predicted

class

cancer =
yes

cancer = no Total

cancer = yes 90 210 300
cancer = no 140 9560 9700
Total 230 9770 10000
----------------------

# Confusion matrix values

TP = 90
FN = 210
FP = 140
TN = 9560

# Calculations

accuracy = (TP + TN) / (TP + TN + FP + FN)
error_rate = (FP + FN) / (TP + TN + FP + FN)
precision = TP / (TP + FP)
recall = TP / (TP + FN)

# Print results

print("Accuracy :", round(accuracy, 4))
print("Error Rate :", round(error_rate, 4))
print("Precision :", round(precision, 4))
print("Recall :", round(recall, 4))

-------------------------------------------------------------
23. With reference to Table , obtain the Frequency table for the
attribute age. From the frequency table you have obtained, calculate
the information gain of the frequency table while splitting on Age. (Use
step by step Python/Pandas commands)
------------------------------------

import pandas as pd
import numpy as np
import math

# 1) Create dataset

data = {
    "Age": ["Young", "Young", "Middle", "Old", "Old", "Old", "Middle", "Young", "Young", "Old", "Young", "Middle", "Middle", "Old"],
    "Income": ["High","High","High","Medium","Low","Low","Low","Medium","Low","Medium","Medium","Medium","High","Medium"],
    "Married": ["No","No","No","No","Yes","Yes","Yes","No","Yes","Yes","Yes","No","Yes","No"],
    "Health": ["Fair","Good","Fair","Fair","Fair","Good","Good","Fair","Fair","Fair","Good","Good","Fair","Good"],
    "Class": ["No","No","Yes","Yes","Yes","No","Yes","No","Yes","Yes","Yes","Yes","Yes","No"]
}

df = pd.DataFrame(data)

# 2) Frequency table for Age

freq_age = df['Age'].value_counts()
print("Frequency table for Age:\n", freq_age)

# 3) Entropy function

def entropy(labels):
    total = len(labels)
    counts = labels.value_counts()
    ent = 0.0
    for count in counts:
        p = count / total
        ent -= p * math.log2(p) if p > 0 else 0
    return ent

# 4) Entropy of whole dataset

total_entropy = entropy(df['Class'])
print("\nEntropy of the whole dataset:", round(total_entropy, 4))

# 5) Entropy for each Age group and weighted sum

weighted_entropy = 0.0
for age, subset in df.groupby('Age'):
    subset_entropy = entropy(subset['Class'])
    weight = len(subset) / len(df)
    weighted_entropy += weight * subset_entropy
    print(f"Entropy of Age = {age}: {round(subset_entropy, 4)}, Weight = {round(weight, 4)}")

# 6) Information Gain for Age

info_gain_age = total_entropy - weighted_entropy
print("\nInformation Gain for splitting on Age:", round(info_gain_age, 4))

-------------------------------------------------------------------------------------------------
24. Perform the following operations using Python on a suitable data set,
counting unique values of data, format of each column, converting
variable data type (e.g. from long to short, vice versa), identifying missing
values and filling in the missing values.
---------------------------------------------


import pandas as pd
import numpy as np

# 1. LOAD DATASET

# Replace 'data.csv' with your file name
df = pd.read_csv("data.csv")


print(df.head())
print("\n")


# 2. FORMAT OF EACH COLUMN (DATA TYPES)

print("===== DATA TYPES OF EACH COLUMN =====")
print(df.dtypes)
print("\n")

print("===== DETAILED INFO OF DATAFRAME =====")
print(df.info())
print("\n")


# 3. COUNT UNIQUE VALUES

print("===== UNIQUE VALUES PER COLUMN =====")
print(df.nunique())
print("\n")

# Example: Unique values and frequency in a specific column
# Change 'Gender' to any column in your dataset

if "Gender" in df.columns:
    print("===== UNIQUE VALUES IN 'Gender' COLUMN =====")
    print(df["Gender"].unique())
    print("\n")

    print("===== VALUE COUNTS OF 'Gender' =====")
    print(df["Gender"].value_counts())
    print("\n")
else:
    print("Column 'Gender' not found. Skipping unique-frequency display for 'Gender'.\n")


# 4. CONVERTING VARIABLE DATA TYPES
#    (EDIT COLUMN NAMES AS PER YOUR DATA)


# Example: convert int64 -> int32 (long to short)

if "Age" in df.columns:
    print("Data type of 'Age' BEFORE:", df["Age"].dtype)
    df["Age"] = df["Age"].astype("int32")
    print("Data type of 'Age' AFTER:", df["Age"].dtype)
    print("\n")
else:
    print("Column 'Age' not found. Skipping int64 -> int32 conversion.\n")

# Example: convert to float64

if "Salary" in df.columns:
    print("Data type of 'Salary' BEFORE:", df["Salary"].dtype)
    df["Salary"] = df["Salary"].astype("float64")
    print("Data type of 'Salary' AFTER:", df["Salary"].dtype)
    print("\n")
else:
    print("Column 'Salary' not found. Skipping conversion to float.\n")

# Example: convert to string (object)

if "ID" in df.columns:
    print("Data type of 'ID' BEFORE:", df["ID"].dtype)
    df["ID"] = df["ID"].astype("string")
    print("Data type of 'ID' AFTER:", df["ID"].dtype)
    print("\n")
else:
    print("Column 'ID' not found. Skipping conversion to string.\n")

# Example: convert to datetime

if "Join_Date" in df.columns:
    print("Data type of 'Join_Date' BEFORE:", df["Join_Date"].dtype)
    df["Join_Date"] = pd.to_datetime(df["Join_Date"], errors="coerce")
    print("Data type of 'Join_Date' AFTER:", df["Join_Date"].dtype)
    print("\n")
else:
    print("Column 'Join_Date' not found. Skipping conversion to datetime.\n")


# 5. IDENTIFY MISSING VALUES

print("===== MISSING VALUES PER COLUMN (BEFORE FILLING) =====")
print(df.isnull().sum())
print("\n")

print("Any missing values in entire DataFrame? ->", df.isnull().values.any())
print("\n")


# 6. FILL MISSING VALUES
#    NUMERIC: mean / median
#    CATEGORICAL: mode


# Separate numeric and non-numeric columns

numeric_cols = df.select_dtypes(include=["int16", "int32", "int64", "float32", "float64"]).columns
categorical_cols = df.select_dtypes(include=["object", "string", "category"]).columns

# ---- Fill numeric columns missing values with mean ----
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna(df[col].mean())

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]   # most frequent value
        df[col] = df[col].fillna(mode_value)

print("===== MISSING VALUES PER COLUMN (AFTER FILLING) =====")
print(df.isnull().sum())
print("\n")


# 7. (OPTIONAL) SAVE CLEANED DATASET

df.to_csv("data_cleaned.csv", index=False)
print("Cleaned dataset saved as 'data_cleaned.csv'")




-------------------------------------------------------------------------------------------------
25. Perform Data Cleaning, Data transformation using Python on any data
set.  (For House Dataset )
---------------------------------------------

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# 1. Load dataset
df = pd.read_csv("House Data.csv")
print("\nOriginal Data:")
print(df.head())

# 2. Remove duplicate rows (noise)
df = df.drop_duplicates()

# 3. Missing value handling
num_cols = df.select_dtypes(include=["int64", "float64"]).columns
cat_cols = df.select_dtypes(include=["object"]).columns

df[num_cols] = df[num_cols].fillna(df[num_cols].median())
for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

# 4. Outlier handling (IQR method for all numeric columns)
for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] < lower, lower, df[col])
    df[col] = np.where(df[col] > upper, upper, df[col])

# 5. Data Transformation
# a) One-hot encoding
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# b) Feature scaling
scaler = MinMaxScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

print("\nCleaned & Transformed Data:")
print(df.head())

# 6. Save final dataset
df.to_csv("Final_House_Data_Cleaned.csv", index=False)
print("\nFile saved as Final_House_Data_Cleaned.csv")


